{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c388a8-6d3d-41b4-b1e8-53ef499fba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --ignore-installed sentencepiece langchain huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4864926-836c-4ac2-a945-5a6851992723",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea416a3-24fb-4b96-adba-e0fd286313ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b79c2-9147-4e26-915a-0f98199e355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, logging\n",
    "import textwrap\n",
    "import locale\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import snapshot_download\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c4ba3-65e6-48c7-9ce3-2732e574afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53c905-24b8-4a58-b6fb-211525edd08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0200225-9d77-4de7-a684-ef61d39c86c4",
   "metadata": {},
   "source": [
    "## Upload model on hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87c86593-bfee-46f2-a858-11e66689cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'models--sambanovasystems--SambaLingo-Arabic-Base/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# rm -r models--sambanovasystems--SambaLingo-Arabic-Base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aec3b-f34d-4466-af1c-7766b076c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"fireworks-ai/firefunction-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb4496-a28b-4a6f-89da-002c053796fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = snapshot_download(repo_id=model_id, cache_dir=\"/workspace/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f4073-55b0-468a-bd31-4f0020fb67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspace/axolotl/src/out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77864c8-ec3e-4f78-a7f9-53a9d2bfa6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eedfc2-5a13-4123-abc4-5d175fbe5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282f9db-cd8c-4224-a168-6b721cca97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, trust_remote_code=True)\n",
    "# tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'bos_token':'<|im_start|>system'},)\n",
    "# tokenizer.add_special_tokens({'eos_token':'<|im_end|>'})\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|im_end|>'})\n",
    "# tokenizer.add_tokens(['<|im_start|>system', '<|im_start|>user','<|im_start|>assistant', '<<SYS>>', '[INST]', '[/INST]'], special_tokens=True)\n",
    "print(\"Model & tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77130dbd-d99c-49ea-82a0-d4ba8caafb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2552f1-a102-4778-a470-718900d8ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"qxsecureserver/tool-selection\", max_shard_size=1024, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a1f68-a96c-4ed6-a497-d59bcbeef410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(\"qxsecureserver/tool-selection\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d259b79-936d-49ac-b97f-7b935cdb66fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "614f2197-d752-4e91-9259-3cc0030e52c2",
   "metadata": {},
   "source": [
    "## Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcf0d810-2c03-4bac-9013-1a5d1fb25d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = \"01-ai/Yi-6B\"\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'bos_token':'<s>'},)\n",
    "tokenizer.add_special_tokens({'eos_token':'</s>'})\n",
    "tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "tokenizer.add_tokens(['<|im_start|>', '<|im_start|>system', '<|im_start|>user','<|im_start|>assistant'], special_tokens=True)\n",
    "print(\"Model & tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55a16f-2b62-4c82-883c-1535eaa000f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550a8d1-c182-4360-b987-aa158e2d9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|im_start|>assistant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2abe3-ac16-4e83-8bc0-948e658091cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_ids = []\n",
    "\n",
    "# labels = []\n",
    "\n",
    "# tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8beae4d-eb00-4585-bb6f-fc5cad2f1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = []\n",
    "\n",
    "# labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15523216-5d92-49bf-84c6-36ca4bd8e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for label in labels:\n",
    "#     if label == -100:\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# first_input_ids = input_ids[:count]\n",
    "# rest_input_ids = input_ids[count:]\n",
    "\n",
    "# # Initialize variables\n",
    "# split_input_ids = [[]] # To store the resulting lists\n",
    "# current_count = 0 # To count the consecutive labels\n",
    "# last_label = -100\n",
    "# # Loop through labels to determine splits\n",
    "# index = 0\n",
    "# for label in labels:\n",
    "#     if label == -100:\n",
    "#         if last_label != -100:\n",
    "#             split_input_ids.append([])\n",
    "#         split_input_ids[-1].append(input_ids[index])\n",
    "#         last_label = label\n",
    "#     else:\n",
    "#         if last_label == -100:\n",
    "#             split_input_ids.append([])\n",
    "#         split_input_ids[-1].append(input_ids[index])\n",
    "#         last_label = label\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9750cc7-bf3a-4b8d-a305-2114e11d5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in split_input_ids:\n",
    "    print(tokenizer.decode(split))\n",
    "    print(\"\\n\\n------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2893b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "# history.append({'role': 'user', 'content': 'Who are you? and who made you?'})\n",
    "# history.append({'role': 'assistant', 'content': 'I am AskQx AI Assistant, a chatbot designed and developed by the team at QXLABAI. I am built to help users with various tasks such as providing information, answering questions, and engaging in conversations.'})\n",
    "# history.append({'role': 'user', 'content': 'What is the 6 most famous song of all time?'})\n",
    "# history.append({'role': 'assistant', 'content': '''Determining the \"most famous\" or most popular songs can be subjective, but some well-known songs that have had significant cultural impact include:\n",
    "\n",
    "# 1. \"Bohemian Rhapsody\" by Queen\n",
    "# 2. \"Imagine\" by John Lennon\n",
    "# 3. \"Like a Rolling Stone\" by Bob Dylan\n",
    "# 4. \"Billie Jean\" by Michael Jackson\n",
    "# 5. \"Stairway to Heaven\" by Led Zeppelin\n",
    "# 6. \"Smells Like Teen Spirit\" by Nirvana\n",
    "\n",
    "# These are just a few examples; there are countless other iconic songs from different genres and eras that could also be considered famous or influential.'''})\n",
    "history.append({'role': 'user', 'content': 'Solve 25-4*2+3'})\n",
    "history.append({'role': 'assistant', 'content': '''25 - 4 * 2 + 3 = \n",
    "\n",
    "First, perform the multiplication:\n",
    "\n",
    "25 - 8 + 3\n",
    "\n",
    "Now, subtract:\n",
    "\n",
    "17 + 3 = 20\n",
    "\n",
    "Finally, add the numbers:\n",
    "\n",
    "20\n",
    "\n",
    "So, the result of 25 - 4 * 2 + 3 is 20.'''})\n",
    "# history.append({'role': 'user', 'content': 'Add 3 more in it'})\n",
    "# history.append({'role': 'assistant', 'content': '7'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58144d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are AskQx AI Assistant developed by QXLABAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e7581-2bc6-4fbd-9346-212f68e5904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_aptchat(prompt, history=history):\n",
    "    new_prompt = f'''{tokenizer.bos_token}<|im_start|>system\\n{system}<|im_end|>\\n'''\n",
    "    for item in history:\n",
    "        if item['role'] == 'user':\n",
    "            new_prompt += f\"<|im_start|>user\\n{item['content']}<|im_end|>\\n\"\n",
    "        else:\n",
    "            new_prompt += f\"<|im_start|>assistant\\n{item['content']}<|im_end|>\\n\"\n",
    "    new_prompt += f\"<|im_start|>user\\n{prompt}<|im_end|>\\n\"\n",
    "    new_prompt += f\"<|im_start|>assistant\\n\"\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_aptchat2(prompt, history=history):\n",
    "    new_prompt = f'''{tokenizer.bos_token}<|system|> {system}{tokenizer.eos_token}'''\n",
    "    for item in history:\n",
    "        if item['role'] == 'user':\n",
    "            new_prompt += f\"<|user|> {item['content']}{tokenizer.eos_token}\"\n",
    "        else:\n",
    "            new_prompt += f\"<|assistant|> {item['content']}{tokenizer.eos_token}\"\n",
    "    new_prompt += f\"<|user|> {prompt}{tokenizer.eos_token}\"\n",
    "    new_prompt += f\"<|assistant|> \"\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a84a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_aptchat3(prompt, history=history):\n",
    "    new_prompt = f'''{tokenizer.bos_token}<|im_start|>system\\n{system}{tokenizer.eos_token}\\n'''\n",
    "    for item in history:\n",
    "        if item['role'] == 'user':\n",
    "            new_prompt += f\"<|im_start|>user\\n{item['content']}{tokenizer.eos_token}\\n\"\n",
    "        else:\n",
    "            new_prompt += f\"<|im_start|>assistant\\n{item['content']}{tokenizer.eos_token}\\n\"\n",
    "    new_prompt += f\"<|im_start|>user\\n{prompt}{tokenizer.eos_token}\\n\"\n",
    "    new_prompt += f\"<|im_start|>assistant\\n\"\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0017db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_aptchat2('''add 2 in it''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff040e6-c568-4260-a6cd-4b310a39807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "stream_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.15,\n",
    "    batch_size=8,\n",
    "    streamer=streamer,\n",
    ")\n",
    "stream_llm = HuggingFacePipeline(pipeline=stream_pipeline)\n",
    "stream_template = f'''User: '''\n",
    "stream_template += \"{query}\"\n",
    "stream_template += f'''Assistant: '''\n",
    "\n",
    "stream_prompt = PromptTemplate(template=stream_template, input_variables=[\"query\"])\n",
    "stream_llm_chain = LLMChain(prompt=stream_prompt, llm=stream_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37492b74-13ec-4f84-b250-841887d238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_llm_chain.run(prompt_aptchat2('''Who are you? and who made you?'''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
